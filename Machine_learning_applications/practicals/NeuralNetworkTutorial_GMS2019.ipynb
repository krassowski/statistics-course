{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkTutorial-GMS2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpwhalley/GMS_Stats_Course/blob/master/6_Machine_Learning_Applications/NeuralNetworkTutorial_GMS2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x30sRRVu_hXA",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network tutorial\n",
        "\n",
        "In this tutorial we will be exploring a number of topics discussed in this morning's lecture, using a famous dataset, the MNIST collection of 70000 handwritten and labeled digits.  This will hopefully make these topics \"come alive\" by showing how they work in practice in a simple but realistic setting.\n",
        "\n",
        "This tutorial makes use of a bunch of useful tools and frameworks: Keras, TensorFlow, Tensorboard, Numpy, Matplotlib, Python, jupyter, Google colab.  You can run this tutorial even if you haven't had previous experience with some or any of these.  Hopefully this tutorial gives you a starting point for exploring them further; in my experience they can massively boost your productivity in data exploration and research generally, and machine learning in particular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsa9KZXxBAzZ",
        "colab_type": "text"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "This just loads the various packages we'll be using, and the MNIST data set.  \n",
        "\n",
        "Make sure you run this on a GPU backend (runs faster) - go to Edit > Notebook settings or Runtime > Change runtime type and select GPU as Hardware accelerator. If you want to double-check, uncomment the line with `device_lib.list_local_devices()` and check that the output shows `device_type GPU` somewhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S1ayS227InJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "#from tensorflow.python.client import device_lib\n",
        "#print(device_lib.list_local_devices())\n",
        "\n",
        "dataformat=\"channels_last\"\n",
        "shape=[28,28,1]\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "print(\"Training: images\",x_train.shape, \" labels\", y_train.shape, \"; Test set\",x_test.shape, \" labels\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70KhZjCNCEHM",
        "colab_type": "text"
      },
      "source": [
        "Always good practice to have a look at your data.  Here are the first two data points, plotted as a matrix, and their labels.\n",
        "\n",
        "The data are just `numpy` arrays; their dimensions are printed above.  Visualisation is done using the `matplotlib` library.\n",
        "\n",
        "*  Find a way to plot several digits with their labels in a grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p934ZSvt7NEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.matshow(x_train[0])\n",
        "plt.matshow(x_train[1])\n",
        "print(y_train[0:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPg68VzeCi0m",
        "colab_type": "text"
      },
      "source": [
        "## Model 1: multinomial logistic regression\n",
        "\n",
        "Below is the first complete model to predict the label (0 to 9) from the pixel intensities, in the simplest possible way (kindof) - using a multinomial logistic regression model, basically logistic regression but for classification instead of binary outcomes.  \n",
        "\n",
        "(The `Sequential` refers to the fact that in this model (and all models in this tutorial), data flows sequentially through a number of layers from input to output.  This is not true for e.g. the Inception module, but Tensorflow/Keras also easily copy with such models.)\n",
        "\n",
        "In detail, for an input $x \\in R^{28\\times 28} = R^{784}$, a vector representing the input image, the model predicts \n",
        "\n",
        "$$\\hat{y} = A x,$$ \n",
        "\n",
        "where $A \\in R^{10 \\times 784}$ and $\\hat{y}\\in R^{10}$.  These are transformed into probabilities using the softmax function:\n",
        "\n",
        "$$p(\\mathrm{digit}=i) = { \\exp(\\hat{y}_i) \\over \\sum_j \\exp(\\hat{y}_j) }$$\n",
        "\n",
        "To compare predicted probabilities $p$ for a given input $x_j$ with the actual class $y_j$, we use the `sparse_categorical_entropy` loss function.  (Here, `sparse` refers to the class encoding we use; we encode classes as integers 0 to 9, rather than \"one-hot encoding\", where we would represent e.g. class 2 as the vector $[0,1,0,0,0,0,0,0,0,0,0]$.)\n",
        "\n",
        "For this model there is theory about how to obtain optimal parameters $A$ for a given data set; but in the spirit of neural networks, we are using a standard stochastic gradient descend algorithm, `adam`.  We will very quickly leave theory behind when we add even a little more complexity to the model, and then numerical optimization is the only option.\n",
        "\n",
        "The call to `compile` builds a computational graph of the model.  The computation performed by the graph includes:\n",
        "\n",
        "1.  evaluation of the model's prediction $F_\\theta(x)$ over a batch $\\{x_1,\\ldots,x_B\\}$; \n",
        "2. calculation of the loss $L_i := L( F_\\theta(x_i), y_i )$;\n",
        "3. calculation of the gradient $\\nabla_\\theta \\sum_{i=1}^L L_i$, and\n",
        "4. a single step of the optimization algorithm `adam`, resulting in new parameters $\\theta$, ready to be executed on a GPU once you feed it data.  \n",
        "\n",
        "If you've ever any of those steps by hand, you will appreciate how transformative Tensorflow/Keras (and similar frameworks) have been in machine learning research.\n",
        "\n",
        "The call to `fit` finally runs the computational graph on successive batches of data.  One complete feed-through of all data is called an `epoch` in machine learning parlance.  The `fit` function is also given test data -- this is not used for training, but to evaluate the model's performance after each epoch.\n",
        "\n",
        "*  How many parameters does this model have?\n",
        "*  Find out how to print a summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvffNZdF764J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()   ## not entirely sure why this is necessary\n",
        "\n",
        "## a very simple model - multinomial logistic regression (= softmax activation) on all 28 x 28 pixels\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "## set up logging via tensorboard\n",
        "logdir = \"logs_model1/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "print(\"Writing log files to \",logdir)\n",
        "tensorboard = TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "## pull the model, loss function, optimizer, and output metrics into a single computational graph\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "## run the computational graph 50 times on the entire training data, to fit the model parameters\n",
        "model.fit(x_train,y_train,epochs=50,validation_data=(x_test,y_test),callbacks=[tensorboard])\n",
        "\n",
        "## evaluate the final model on the test set\n",
        "print(\"Test set performance:\")\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd419VmzKbVz",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard\n",
        "\n",
        "The `tensorboard` \"callback\" in the code above is called by the `fit` function at the end of each epoch.  The callback writes some logging information in a directory, which we can explore later.\n",
        "\n",
        "Tensorboard is a standalone program that visualises the log files produced by tensorflow.  Running the cell below starts the tensorboard program inside the notebook.  You just have to do this once and use it for all runs in this session. Within tensorboard you can select the log files you want to look at.\n",
        "\n",
        "Once the run above has finished, start Tensorboard and have a look at the run.  Notice that the model has overtrained - the validation loss has reached a minimum at around the 10th epoch, and gradually increased after that, even as the training loss continued to decrease.  This behaviour is seen quite often (but more usually after many more epochs).  Early Stopping is a heuristic that identifies the validation loss minimum, and stops training there.\n",
        "\n",
        "*  Click on the 'graph' tab.  Can you make sense of the computational graph? Try double-clicking on nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKEt_1VPA5kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load TENSORBOARD\n",
        "%load_ext tensorboard\n",
        "# Start TENSORBOARD\n",
        "%tensorboard --logdir logs_model1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcQqW0aHO97y",
        "colab_type": "text"
      },
      "source": [
        "## Model 2 - a two-layer neural network\n",
        "\n",
        "The accuracy (about 90%) achieved by the first model is not bad, but not hugely impressive either.  Part of the reason is that the model is purely a linear model, without nonlinearities or interactions.  So let's add a second layer of neurons, with the standard (ReLU) activation.\n",
        "\n",
        "*  Compare this model's performance with that of model 1\n",
        "*  Also compare the extent of overtraining that has occurred.\n",
        "*  How many parameters does this model have?\n",
        "*  Try running with fewer or more than 50 nodes in the middle layer.  How does this affect the model's performance and degree of overtraining?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KYymOvMRO96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## a very simple model - multinomial logistic regression (= softmax activation) on all 28 x 28 pixels\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(50,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "logdir = \"logs_twolayers/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard = TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=25,validation_data=(x_test,y_test),callbacks=[tensorboard])\n",
        "print(\"Test set performance:\")\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OyXIwx7RsfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## tensorboard does not seem to find new data in old log directories, so start a new instance\n",
        "%tensorboard --logdir logs_twolayers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2Yqj2DAasI",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional neural network\n",
        "\n",
        "Adding a second layer of neurons did improve performance a bit, but we're a long way off state of art.  A key shortcoming of the \n",
        "models so far is that they treat pixels in isolation.  In particular the models are not constrained to be translation-symmetric: shifting the input image by one pixel to the right should not make a difference in the output.\n",
        "\n",
        "One way to help the model is to use a convolutional layer, which re-uses the same \"kernel\" and applies it across the (2D) image.\n",
        "The next model uses two 2D convolutions, each using 'same' padding which ensures the output resolution is the same as the input resolution.  Each 2D convolution is followed by a $2\\times 2$ max pooling layer, reducing the resolution with a factor 2 in both dimensions.\n",
        "\n",
        "The first convolutional layer has 32 kernels, so its output is of dimension $28\\times 28\\times 32$.  After max pooling this becomes $14 \\times 14\\times 32$.  The next convolutional layer has 64 kernels, and after max pooling the output dimension is $7\\times 7\\times 64$.  This is then followed by a $50$-channel fully connected layer, and finally a $10$-channel output layer.\n",
        "\n",
        "*  How many parameters does this model have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1LS7Xc-T1Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## a simple convolutional model:\n",
        "## - 2d convolutional layer with 32 5x5 kernels, and ReLU activation (here implemented as a separate layer)\n",
        "## - 2d max pooling, input size 2x2, and stride 2 in both dimensions\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Reshape(target_shape=shape, input_shape=[28,28]),\n",
        "    tf.keras.layers.Conv2D(16, 5, padding='same', data_format=dataformat, activation='linear'),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2),(2,2), padding='same', data_format=dataformat),\n",
        "    tf.keras.layers.Conv2D(16, 5, padding='same', data_format=dataformat, activation='linear'),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2),(2,2), padding='same', data_format=dataformat),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(50,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "logdir = \"logs_twoconvolutions/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard = TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test),callbacks=[tensorboard])\n",
        "print(\"Test set performance:\")\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llc2_oEpyCyy",
        "colab_type": "text"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "The convolutional model is pretty good, but still suffers from overtraining.  The trailing loss has become very low, while the\n",
        "test accuracy stabilises.\n",
        "\n",
        "Dropout randomly removes neurons from the network (sets the corresponding output to 0).  The probability that a neuron\n",
        "is dropped out is set by the user.  It can be shown that the\n",
        "effect of this is (approximately) equivalent to putting a \n",
        "prior on the parameters, causing them to shrink, which reduces\n",
        "overtraining.\n",
        "\n",
        "*  Does this address overtraining?  Why is the prediction accuracy higher than the training accuracy?  (Prediction is deterministic; how is this achieved with dropout layers?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjzQCOpNFU3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "dataformat=\"channels_last\"\n",
        "shape=[28,28,1]\n",
        "\n",
        "## a simple convolutional model:\n",
        "## - 2d convolutional layer with 32 5x5 kernels, and ReLU activation (here implemented as a separate layer)\n",
        "## - 2d max pooling, input size 2x2, and stride 2 in both dimensions\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Reshape(target_shape=shape, input_shape=[28,28]),\n",
        "    tf.keras.layers.Conv2D(16, 5, padding='same', data_format=dataformat, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2),(2,2), padding='same', data_format=dataformat),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Conv2D(16, 5, padding='same', data_format=dataformat, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2),(2,2), padding='same', data_format=dataformat),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(50,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "logdir = \"logs_twoconvolutions_dropout/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard = TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=20,validation_data=(x_test,y_test),callbacks=[tensorboard])\n",
        "print(\"Test set performance:\")\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q65FgZ1HE_yp",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "The first convolutional layer shares weights across positions of the image.  It detects features like edges and corners, and this\n",
        "weight sharing encodes our intuition that these features can occur\n",
        "anywhere in the image.\n",
        "\n",
        "More broadly, our model should interpret images of digits in the same may, no matter where the digits occur within the image.  In other words it should be \"invariant\" with respect to translations.\n",
        "A convolutional layer helps to achieve this - shifting the input by one pixel, causes the output of the convolutional layer to also shift by one pixel. (This is not \"invariance\", but rather \"equivariance\" - the output is transformed by an \"equivalent\" transformation.)  However, the other layers (e.g. the max pool layer) break the symmetry again -- shifting the image by one pixel to the left, will give an output at the max pool layer that does  not correspond in a simple way to the original output.\n",
        "\n",
        "Data augmentation helps to make the model more symmetric, by giving it more \"equivalent\" input data points, and let the model learn the required symmetry.\n",
        "\n",
        "Rotational symmetry is another symmetry of the model - digits should be interpreted the same way if they are rotated by a small angle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk5CAXJ1GmPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment_data(dataset, dataset_labels, num_augmented_images=1):\n",
        "  augmented_images = []\n",
        "  augmented_image_labels = []\n",
        "  dataset_with_colourchan = np.reshape(dataset, dataset.shape + (1,))\n",
        "  for num in range (dataset.shape[0]):\n",
        "      ## original image\n",
        "      augmented_images.append(dataset[num])\n",
        "      augmented_image_labels.append(dataset_labels[num])\n",
        "      \n",
        "      for i in range(num_augmented_images):\n",
        "        ## shift images by up to 0.05*28 (~1) pixels in any direction\n",
        "        augmented_image = tf.contrib.keras.preprocessing.image.random_shift(dataset_with_colourchan[num], 0.05, 0.05, \n",
        "                                                                            row_axis=0, col_axis=1, channel_axis=2)\n",
        "        ## add augmented image, dropping the colour channel again\n",
        "        augmented_images.append( augmented_image[:,:,0] )\n",
        "        augmented_image_labels.append(dataset_labels[num])\n",
        "\n",
        "        ## rotate images by up to 20 degrees\n",
        "        augmented_image = tf.contrib.keras.preprocessing.image.random_rotation(dataset_with_colourchan[num], 20, \n",
        "                                                                               row_axis=0, col_axis=1, channel_axis=2)\n",
        "        augmented_images.append( augmented_image[:,:,0] )\n",
        "        augmented_image_labels.append(dataset_labels[num])\n",
        "\n",
        "\n",
        "  return np.array(augmented_images), np.array(augmented_image_labels)\n",
        "\n",
        "## This creates an augmented dataset in memory.  This can also be done on the fly\n",
        "## using ImageDataGenerator, but it turns out that's quite slow in this case.\n",
        "x_aug_train, y_aug_train = augment_data(x_train, y_train, 1)\n",
        "\n",
        "## Have a look at the first digit and its transformations (translation and rotation)\n",
        "plt.matshow(x_aug_train[0])\n",
        "plt.matshow(x_aug_train[1])\n",
        "plt.matshow(x_aug_train[2])\n",
        "print(y_aug_train[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1OHg1NwLlNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## the same model as above\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Reshape(target_shape=shape, input_shape=[28,28]),\n",
        "    tf.keras.layers.Conv2D(16, 5, padding='same', data_format=dataformat, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2),(2,2), padding='same', data_format=dataformat),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Conv2D(16, 5, padding='same', data_format=dataformat, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2,2),(2,2), padding='same', data_format=dataformat),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(50,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "logdir = \"logs_data_augmentation/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard = TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_aug_train,y_aug_train,epochs=20,batch_size=3*32,validation_data=(x_test,y_test),callbacks=[tensorboard])\n",
        "print(\"Test set performance:\")\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fayiDUEWJAnC",
        "colab_type": "text"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "The model we have now is quite good (I'm getting ~99.5% test accuracy), but there probably still is room for improvement.  It contains quite a few parameters we could tweak; the data augmentation could\n",
        "be refined; we can change the parameters of the optimizer, or the\n",
        "optimizer itself; and we can add or remove layers or other features\n",
        "of the model.\n",
        "\n",
        "This exercise is called \"hyperparameter search\".  Strategies here range from a simple grid search, to empirical modeling of the outcome as a function of the hyperparameters and directing the search in that way.\n",
        "\n",
        "Hyperparameter search is best done on a server or cluster, so\n",
        "we won't try this in this tutorial.\n",
        "\n",
        "Other topics not covered are dilated networks and residual networks, which we have found helpful in modeling DNA.\n",
        "However, hopefully the two tutorials in this section have\n",
        "given you a number of entry points with which to start building\n",
        "your own models.\n",
        "\n",
        "\n"
      ]
    }
  ]
}