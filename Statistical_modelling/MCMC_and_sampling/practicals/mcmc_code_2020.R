# ---------------------
# 1) Introduction to MCMC course.
# --------------------
dev.off()
dev.off()


# ---------------------
# 2) height data
# --------------------
# Download the height data and then read it into R.
h_data = read.csv('./height_data.csv')
N = length(h_data$x)

# estimate mean height
mean(h_data$x)

# estimate the standard deviation of the height.
sd(h_data$x)

# what is the probability that a randomly chosen individual has a heigth of < 165 cm?
sum(h_data$x <165) / N

# What is the probability that a randomly chosen individual has a height in the range [180, 190] cm?
sum(h_data$x > 180 & h_data$x < 190) / N



# ---------------------
# 3) MCMC on a mixture of two normals.
# --------------------
# PDF of target distribution . I am using "dnorm" function from R, 
# but you can write down the formula for the mixture of normals in R as in the presentation.
mu <- c(-1, 3)
sd <- c(2, 1)
p <- function(x) {
  0.5     * dnorm(x, mu[1], sd[1]) +
    0.5 * dnorm(x, mu[2], sd[2])
}

# plot the target distribution.
curve(p(x), col="red", -8, 8, n=301, las=1)

# decide on a proposal distribution: normal with SD=4
q <- function(x) {
  rnorm(n = 1, mean = x, sd = 4)
}

# how many steps do you want to run the algorithm for.
nStep = 1000

# Set starting value for theta.
theta_0 = 10

# set a vector to store the samples generated by the algorithm
thetas = rep(0,nStep)
thetas[1] = theta_0

# Run the metropolis algorithm.
for (i in 2:nStep) {
  # draw a sample from proposal distribution using previous sample.
  theta_star = q(thetas[i-1])
  
  # draw a random sample from uniform(0,1) distribution.
  u = runif(n=1)
  
  # calculate the ratio of the proposed sample to the previous sample
  alpha = p(theta_star) / p(thetas[i-1])
  
  # compare the random uniform number to the ratio and store the next sample as approporiate.
  if (u < alpha) {
    thetas[i] = theta_star
  } else { 
    thetas[i] = thetas[i-1]
  }
}


############### This section is just plotting things.

# make a plot of the samples.
layout(matrix(c(1, 2), 1, 2), widths=c(4, 1))
par(mar=c(4.1, .5, .5, .5), oma=c(0, 4.1, 0, 0))
plot(thetas, type="s", xpd=NA, ylab="Parameter", xlab="Sample", las=1)
usr <- par("usr")
xx <- seq(usr[3], usr[4], length=301)
plot(p(xx), xx, type="l", yaxs="i", axes=FALSE, xlab="")


# make a histogram to compare to the target distribution.
hist(thetas, 50, freq=FALSE, main="", ylim=c(0, .4), las=1,
     xlab="x", ylab="Probability density")
z <- integrate(p, -Inf, Inf)$value
curve(p(x) / z, add=TRUE, col="red", n=200)


# ---------------------
# 3.1) write functions to make running the MCMC easier.
# --------------------
# I am going to write some functions to make running the MCMC much easier. rather than copy pasting the code every time I change something.

# function for the core of the MCMC.
step <- function(x, p, q) {
  ## Pick new point
  xp <- q(x)
  ## Acceptance probability:
  alpha <- min(1, p(xp) / p(x))
  ## Accept new point with probability alpha:
  if (runif(1) < alpha)
    x <- xp
  ## Returning the sampled value:
  return(x)
}

# function to run MCMC steps
run <- function(x, p, q, nSteps) {
  res <- matrix(NA, nSteps, 1)
  for (i in seq_len(nSteps))
    res[i,] <- x <- step(x, p, q)
  drop(res)
}

# ---------------------
# 3.2) Run MCMC for longer.
# --------------------
# Run MCMC for longer and see what the draws look like.
dev.off()
dev.off()

# run for 50000 steps.
res.long <- run(-10, p, q, 500000)
# make plots.
hist(res.long, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
     xlab="x", ylab="Probability density", col="grey")
z <- integrate(p, -Inf, Inf)$value
curve(p(x) / z, add=TRUE, col="red", n=200)

# remove the first 5000 steps as burn-in. To thin use every 50th step.
res.long.burn.thin = res.long[seq(5001,length(res.long),by=50)]
length(res.long.burn.thin)

# plot figure for the chain after burn-in and thinning.
hist(res.long.burn.thin, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
     xlab="x", ylab="Probability density", col="grey")
z <- integrate(p, -Inf, Inf)$value
curve(p(x) / z, add=TRUE, col="red", n=200)

# look at autocorrelation of the two chains.
par(mfrow=c(1, 2), mar=c(4, 2, 3.5, .5))
acf(res.long,100, las=1, main="original chain")
acf(res.long.burn.thin,100, las=1, main="after burn-in and thinning")

# ---------------------
# 3.3) proposal distributions and their impact on the chain.
# --------------------
# let's change the proposal distribution and see what the draw look like.
# There is only one parameter in our proposal distribution that we can change the variance (or SD).

# large moves: A large variance means the proposed values will be far away from the current value.
res.fast <- run(-10, p, function(x) rnorm(1, mean = x,  sd = 33), 1000)

# small moves: A small variance means the proposed values are not that different from the current value.
res.slow <- run(-10, p, function(x) rnorm(1, mean = x,  sd = .3), 1000)


## make plots of the results to see what they look like:
layout(matrix(c(1, 2), 1, 2), widths=c(4, 1))
par(mar=c(4.1, .5, .5, .5), oma=c(0, 4.1, 0, 0))
plot(thetas, type="s", xpd=NA, ylab="Parameter", xlab="Sample", las=1,
     col="grey",ylim=c(-10,10))
lines(res.fast, col="red")
lines(res.slow, col="blue")
usr <- par("usr")
xx <- seq(usr[3], usr[4], length=301)
plot(p(xx), xx, type="l", yaxs="i", axes=FALSE)



# calculate the acceptance rate for each MCMC chain.
library(coda)

# for large moves.
mcmc.trace.fast <- mcmc(res.fast)
#  summary(mcmc.trace)
1-rejectionRate(mcmc.trace.fast)

# for small moves
mcmc.trace.slow <- mcmc(res.slow)
# summary(mcmc.trace)
1-rejectionRate(mcmc.trace.slow)

# for intermediate moves
mcmc.trace <- mcmc(thetas)
# summary(mcmc.trace)
1-rejectionRate(mcmc.trace)

# plot autocorrelation
par(mfrow=c(1, 3), mar=c(4, 2, 3.5, .5))
acf(res.slow,100, las=1, main="Small steps")
acf(thetas,100, las=1, main="Intermediate")
acf(res.fast,100, las=1, main="Large steps")


# ---------------------
# 4) MCMC for the number of de novo mutations
# --------------------
# the data. mutation in children's genomes.
pois_data = c(42, 49, 40, 36, 50, 37, 39, 43, 54, 55)

# target distribution. I have ignored the normalising constant
p <- function(lambda, pois_data){
  if (lambda >= 0  & lambda <= 1000){
    lambda^(sum(pois_data)) * exp(-length(pois_data) *lambda)
  }else{
    0
  }
}

# does the above work?
p(10,pois_data)

# We get an overflow error in the above. So need to work in the log space.
logp <- function(lambda, pois_data){
  if (lambda >= 0  & lambda <= 1000){
    (sum(pois_data) * log(lambda)) -(length(pois_data) * lambda)
  }else{
    0
  }
}

# decide on a proposal distribution:
q <- function(x) {
  rnorm(n = 1, mean = x, sd = 1)
}

# how many steps do you want to run the algorithm for.
nStep = 10000

# Set starting value for theta.
lambda_0 = 10

# set a vector to store the samples generated by the algorithm
lambdas = rep(0,nStep)
lambdas[1] = lambda_0

# Run the metropolis algorithm.
for (i in 2:nStep) {
  # draw a sample from proposal distribution using previous sample.
  lambda_star = q(lambdas[i-1])
  
  # draw a random sample from uniform(0,1) distribution.
  u = runif(n=1)
  
  # calculate the ratio of the proposed sample to the previous sample
  log_alpha = logp(lambda_star,pois_data) - logp(lambdas[i-1],pois_data)
  
  # compare the random uniform number to the ratio and store the next sample as approporiate.
  if (log(u) < log_alpha) {
    lambdas[i] = lambda_star
  } else { 
    lambdas[i] = lambdas[i-1]
  }
}

############# make a plot
layout(matrix(c(1, 2), 2, 2), widths=c(2, 2))
par(mar=c(4.1, .5, .5, .5), oma=c(0, 4.1, 0, 0))
hist(lambdas, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
     xlab="x", ylab="Probability density", col="grey")
plot(lambdas, type="l", xpd=NA, ylab="Parameter", xlab="Sample", las=1,
     col="red",ylim=c(10,60))

###### what is the probability that mutation rate > 45 mutations per genome.
final_res = lambdas[seq(1000,length(lambdas),10)]
length(final_res)

sum(final_res > 45) / length(final_res)


# ---------------------
# 5) MCMC for allele frequency
# --------------------

# prior distribution on p.
prior = function(p){
  if((p<0) || (p>1)){  # || here means "or"
    return(0)}
  else{
    return(1)}
}

# likelihood function.
likelihood = function(p, nAA, nAa, naa){
  return(p^(2*nAA) * (2*p*(1-p))^nAa * (1-p)^(2*naa))
}

p_sampler = function(nAA, nAa, naa, n_iter, p_start_val, pproposal_sd){
  p = rep(0,n_iter)
  p[1] = p_start_val
  for(i in 2:n_iter){
    currentp = p[i-1]
    newp = rnorm(1,currentp,pproposal_sd)
    A = (prior(newp)*likelihood(newp,nAA,nAa,naa)) / ((prior(currentp) * likelihood(currentp,nAA,nAa,naa)))
    if(runif(1) < A){
      p[i] = newp       # accept move with probability min(1,A)
    } else {
      p[i] = currentp        # otherwise "reject" move, and stay where we are
    }
  }
  return(p)
}


z=p_sampler(50,21,29,10000,0.1,0.01)
dev.off()
plot(z,type='l')

hist(z,prob=T,100,main='')



# -----------------------------------
# 6) MCMC for the allele frequency and the inbreading coef.
# ----------------------------------------
prior_p = function(p){
  if((p<0) || (p>1)){  # || here means "or"
    return(0)}
  else{
    return(1)}
}

prior_g = function(g){
  if((g<0) || (g>1)){  # || here means "or"
    return(0)}
  else{
    return(1)}
}

likelihood = function(p,g, nAA, nAa, naa){
  return((g*p+(1-g)*p^2)^nAA * ((1-g)*2*p*(1-p))^nAa * (g*(1-p)+(1-g)*(1-p)^2)^naa)
}

g_p_sampler = function(nAA, nAa, naa, n_iter, g_start_val, p_start_val, g_proposal_sd, p_proposal_sd){
  g = rep(0,n_iter)
  p = rep(0,n_iter)
  g[1] = g_start_val
  p[1] = p_start_val
  for(i in 2:n_iter){
    current_g = g[i-1]
    current_p = p[i-1]
    new_g = current_g + rnorm(1,0,g_proposal_sd)
    new_p = current_p + rnorm(1,0,p_proposal_sd)
    A = (prior_p(new_p)*prior_g(new_g)*likelihood(new_p,new_g,nAA,nAa,naa)) / 
      ((prior_p(current_p)*prior_g(current_g)*likelihood(current_p,current_g,nAA,nAa,naa)))
    if(runif(1)<A){
      p[i] = new_p
      g[i] = new_g # accept move with probabily min(1,A)
    } else {
      p[i] = current_p        # otherwise "reject" move, and stay where we are
      g[i] = current_g
    }
  }
  return(list(g=g,p=p)) # return a "list" with two elements named f and p
}

# out = g_p_sampler(100,21,49,1e4,0.1,0.1,0.1,0.1)

out = g_p_sampler(50,21,29,1e4,0.1,0.1,0.1,0.1)

par(mfrow=c(2, 2), mar=c(4, 2, 3.5, .5))
plot(out$g,type='n')
lines(out$g)
hist(out$g,30)


plot(out$p,type='n')
lines(out$p)
hist(out$p,30)

dev.off()
smoothScatter(out$g,out$p)
lines(out$g,out$p)

